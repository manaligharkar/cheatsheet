#Procedure to install Azure CLI on unix:
AZ_REPO=$(lsb_release -cs)
echo "deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main" | sudo tee /etc/apt/sources.list.d/azure-cli.list
#Install the gpg key for Microsoft's repository
curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg > /dev/null
sudo apt-get update
sudo apt-get install azure-cli
#Log into our subscription
az login
az account set --subscription "Demonstration Account"  OR az account set -s "Demonstration Account"
#Create a "esource group"for the serivces we're going to create  OR skip if already RG is present-->Optional step
az group create --name "Kubernetes-Cloud" --location centralus
#Let's get a list of the versions available to us
az aks get-versions --location centralus -o table
#Let's create our AKS managed cluster. Use --kubernetes-version to specify a version.
az aks create --resource-group "Kubernetes-Cloud" --generate-ssh-keys --name CSCluster --node-count 3           -->#default Node count is 3
#If needed, we can download and install kubectl on our local system.
az aks install-cli
for windows-->https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/
#Get our cluster credentials and merge the configuration into our existing config file.
#This will allow us to connect to this system remotely using certificate based user authentication.
az aks get-credentials --resource-group "Kubernetes-Cloud" --name CSCluster
#List our currently available contexts
kubectl config get-contexts
#set our current context to the Azure context
kubectl config use-context CSCluster
++++++++++++++++++++++++++++++end of configuation of aks cluster++++++++++++++++++++++++++++++++++++++++++++++
#run a command to communicate with our cluster.
kubectl get nodes
#Get a list of running pods, we'll look at the system pods since we don't have anything running.
#Since the API Server is HTTP based...we can operate our cluster over the internet...esentially the same as if it was local using kubectl.
kubectl get pods --all-namespaces
#Let's set to the kubectl context back to our local custer
kubectl config use-context kubernetes-admin@kubernetes
#use kubectl get nodes
kubectl get nodes
#az aks delete --resource-group "Kubernetes-Cloud" --name CSCluster #--yes --no-wait
==============================================
Monitring AKS
# Set variables to be used in the scripts in other demos
$Region="eastus"
$RG="1-9bee0ce5-playground-sandbox"
$Sub=""
$AKSCluster="AKSCluster"
$AKSCluster_Limited="AKSCluster-Limited"
$AKSCluster_CNI="AKSCluster-CNI"
$ClusterSize="Standard_DS4_v2"
$ClusterSize_Limited="Standard_H8"
$LAWS_Name="AKS-LAWS"

cloud_user@43ab4ac5431c:~$ kubectl get nodes
NAME                                STATUS   ROLES   AGE     VERSION
aks-nodepool1-34259712-vmss000000   Ready    agent   5m52s   v1.20.9
aks-nodepool1-34259712-vmss000001   Ready    agent   4m39s   v1.20.9
aks-nodepool1-34259712-vmss000002   Ready    agent   5m54s   v1.20.9
cloud_user@43ab4ac5431c:~$
# Create Log Analytics Workspace#IMP
az monitor log-analytics workspace create --resource-group $RG --workspace-name $LAWS_Name

Go to browser--AKS cluster-->monitoring-insights-->select drop down--newly created "Log Analytics workspace" i.e.AKS-LAWS--click on Enable.
defaul pod count wil b 15-system pod
# Create Namespace
kubectl create namespace nginx
kubectl config set-context --current --namespace=nginx

# Create a Deployment
kubectl create deployment nginx --image=nginx 
after this on brwoser--u wil see 16 pod

# Scale up
kubectl scale deployment nginx --replicas=20
# Scale down
kubectl scale deployment nginx --replicas=2 
workbook/metrics wil show data
IMP--ALert-->1step-->create action group--email noticification.
2nd step->create alert rule->create condictio like node is in "not ready " --count 1 -->save.
3step--selected--action grop which we created in 1st step.
4th--give name to alert rule and set severity
second way--inside insights--there are Recommended alerts (Preview)
select alert-enable--assign action group.
Logs-->u can run any alert RUN-->chnage the timing in query and export result.
=====================================
Kibana+garfana
kubectl create namespace logging
kubectl config set-context --current --namespace=logging

# Deploy elastic search, fluentd and Kibana
kubectl apply -f es-statefulset.yaml
kubectl apply -f es-service.yaml
kubectl apply -f fluentd-es-configmap.yaml
kubectl apply -f fluentd-es-ds.yaml
kubectl apply -f kibana-deployment.yaml
kubectl apply -f kibana-service.yaml

# Check out deployed pods
kubectl get pods
cloud_user@43ab4ac5431c:~/my-kibana$ kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
elasticsearch-logging-0           1/1     Running   0          4m8s
elasticsearch-logging-1           1/1     Running   0          2m29s
elasticsearch-logging-2           1/1     Running   0          75s
fluentd-es-v3.1.1-2dzqr           1/1     Running   1          3m38s
fluentd-es-v3.1.1-kmwsw           1/1     Running   1          3m38s
fluentd-es-v3.1.1-s7dh2           1/1     Running   1          3m38s
kibana-logging-6778fdcd79-78zcs   1/1     Running   1          3m33s
cloud_user@43ab4ac5431c:~/my-kibana$


# Retrieve service
kubectl get svc kibana-logging
cloud_user@43ab4ac5431c:~/my-kibana$ kubectl get svc kibana-logging
NAME             TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)          AGE
kibana-logging   LoadBalancer   10.0.9.115   13.89.115.144   5601:32612/TCP   4m8s
cloud_user@43ab4ac5431c:~/my-kibana$

http://13.89.115.144:5601
# Check out Kibana
$KibanaURL="http://" + (kubectl get svc kibana-logging -o jsonpath='{.status.loadBalancer.ingress[0].ip}') + ":5601"
Start-Process $KibanaURL
On Kibaban-->create new "Index pattern" where kinaa organaises data
log* and @timestamp

# Sale nginx
kubectl scale deployment nginx --replicas=10 -n nginx

# Create Namespace for Grafana
kubectl create namespace monitoring
kubectl config set-context --current --namespace=monitoring

# Add Helm repos / update Helm repos
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add stable https://charts.helm.sh/stable
helm repo update

# Install Prometheus stack (which includes Grafana!)
helm install monitor prometheus-community/kube-prometheus-stack --namespace monitoring

# Check our Pods and Service
kubectl get pods
kubectl get svc monitor-grafana

# Change Service-Type to LoadBalancer
kubectl patch svc monitor-grafana -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'
kubectl get svc monitor-grafana

# Check our Grafana Portal
# Default credentials:
# Username: admin
# Password: prom-operator
$GrafanaURL="http://" + (kubectl get svc monitor-grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}') 
Start-Process $GrafanaURL

# Scale the nginx deployment
kubectl scale deployment nginx --replicas=15 -n nginx
=========================================
# Create Namespace for Grafana
kubectl create namespace monitoring
kubectl config set-context --current --namespace=monitoring

# Add Helm repos / update Helm repos
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add stable https://charts.helm.sh/stable
helm repo update

# Install Prometheus stack (which includes Grafana!)
helm install monitor prometheus-community/kube-prometheus-stack --namespace monitoring
cloud_user@43ab4ac5431c:~$ kubectl --namespace monitoring get pods -l "release=monitor"
NAME                                                   READY   STATUS    RESTARTS   AGE
monitor-kube-prometheus-st-operator-5c98b65c77-54b7z   1/1     Running   0          65s
monitor-prometheus-node-exporter-2qd8g                 1/1     Running   0          66s
monitor-prometheus-node-exporter-fcf57                 1/1     Running   0          65s
monitor-prometheus-node-exporter-pkn8c                 1/1     Running   0          65s
cloud_user@43ab4ac5431c:~$
cloud_user@43ab4ac5431c:~$ kubectl get pods
NAME                                                     READY   STATUS    RESTARTS   AGE
alertmanager-monitor-kube-prometheus-st-alertmanager-0   2/2     Running   0          115s
monitor-grafana-b9c974489-k4gvf                          3/3     Running   0          2m
monitor-kube-prometheus-st-operator-5c98b65c77-54b7z     1/1     Running   0          2m
monitor-kube-state-metrics-b557b4cdd-28n4k               1/1     Running   0          2m
monitor-prometheus-node-exporter-2qd8g                   1/1     Running   0          2m1s
monitor-prometheus-node-exporter-fcf57                   1/1     Running   0          2m
monitor-prometheus-node-exporter-pkn8c                   1/1     Running   0          2m
prometheus-monitor-kube-prometheus-st-prometheus-0       2/2     Running   0          115s
cloud_user@43ab4ac5431c:~$
=============================================
cloud_user@43ab4ac5431c:~$ kubectl get svc monitor-grafana
NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
monitor-grafana   ClusterIP   10.0.67.46   <none>        80/TCP    2m42s
# Change Service-Type to LoadBalancer
kubectl patch svc monitor-grafana -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'   -->windows
kubectl patch svc monitor-grafana -p '{"spec": {"type": "LoadBalancer"}}'         -->Unix
cloud_user@43ab4ac5431c:~$  kubectl get svc monitor-grafana
NAME              TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)        AGE
monitor-grafana   LoadBalancer   10.0.67.46   20.97.73.151   80:31455/TCP   16m
cloud_user@43ab4ac5431c:~$
cloud_user@43ab4ac5431c:~$ kubectl get svc monitor-grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}'
20.97.73.151
cloud_user@43ab4ac5431c:~$
http://20.97.73.151/--> will give garfana url
If using Prometheus Operator then user/pass is:

user: admin
pass: prom-operator
===========================
To see current namespace
cloud_user@43ab4ac5431c:~$ kubectl config get-contexts --no-headers | grep '*' | grep -Eo '\S+$'   OR kubectl config get-contexts
=============================







